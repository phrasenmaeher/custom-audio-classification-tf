{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custaudio_tfr_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phrasenmaeher/custom-audio-classification-tf/blob/main/custaudio_tfr_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L-j5ZCq92PO"
      },
      "source": [
        "Code for section 3 of the post at\n",
        "[TDS/Medium](https://towardsdatascience.com/custom-audio-classification-with-tensorflow-af8c16c38689)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIsClvq1VVXB"
      },
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "tfr_dir = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEhj_zSFVkDh"
      },
      "source": [
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    if isinstance(value, type(tf.constant(0))): # if value is tensor\n",
        "        value = value.numpy() # get value of tensor\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def serialize_array(array):\n",
        "  array = tf.io.serialize_tensor(array)\n",
        "  return array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqxMzBg3VtRy"
      },
      "source": [
        "def gen_example(sound_clip, sr, label, shape):\n",
        "  data={'x': _int64_feature(shape[0]),\n",
        "        'y': _int64_feature(shape[1]),\n",
        "        'sr':_int64_feature(sr),\n",
        "        'label': _int64_feature(label),\n",
        "        'feature':_bytes_feature(serialize_array(sound_clip))}\n",
        "  \n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfqELWDwVzEk"
      },
      "source": [
        "def gen_tfr(csv_path, outname, max_files):\n",
        "  print(\"Parsing {}\".format(csv_path))\n",
        "  \n",
        "  df = pd.read_csv(csv_path)\n",
        "  df = df.sample(frac=1).reset_index(drop=True) #shuffle the dataset before writing into TFRecords (prevents batches of only one label later on)\n",
        "  \n",
        "  splits = (len(df)//max_files) + 1 #determine how many tfr shards are needed\n",
        "  if len(df)%max_files == 0:\n",
        "    splits-=1\n",
        "\n",
        "\n",
        "  print(f\" Using {splits} shard(s) for {len(df)} files, with up to {max_files} samples per shard\")\n",
        "  file_counter = 0\n",
        "  for i in tqdm.tqdm(range(splits)):\n",
        "    filename = \"{}{}of{}_{}.tfrecords\".format(tfr_dir, i+1, splits, outname) #create TFR shard\n",
        "    writer = tf.io.TFRecordWriter(filename)\n",
        "    \n",
        "    current_shard_count = 0\n",
        "    while current_shard_count < max_files: #while this shard is not full\n",
        "      \n",
        "      index = i*max_files+current_shard_count\n",
        "      if index == len(df): #when we have consumed the whole DF, preempt generation\n",
        "        break\n",
        "      #print(index)\n",
        "\n",
        "      row = df.iloc[index] #get the sample\n",
        "\n",
        "      sound_clip,sr = librosa.load(row[0], sr=22050)\n",
        "      sound_clip = np.expand_dims(sound_clip, axis=1)\n",
        "      if sound_clip.shape[0] != 1323000: #skip any samples that are not of appropriate shape\n",
        "        print(\"{} was not of fit shape: {}\".format(row[0], sound_clip.shape))\n",
        "        current_shard_count += 1\n",
        "        continue\n",
        "\n",
        "      shape = sound_clip.shape\n",
        "      label = row[1]\n",
        "\n",
        "      data = gen_example(sound_clip, sr, label, shape) #generate the example\n",
        "\n",
        "      out = tf.train.Example(features=tf.train.Features(feature=data))\n",
        "      writer.write(out.SerializeToString())\n",
        "      current_shard_count+=1\n",
        "      \n",
        "      file_counter +=1\n",
        "\n",
        "    writer.close()\n",
        "  \n",
        "  print(\"Parsed {} files for {}\".format(str(file_counter), outname))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0qZwI7YXOpl"
      },
      "source": [
        "def gen_monitoring_sample(csv_path, outname, num_samples, use_all):\n",
        "  print(\"Parsing {} to enable logging some statistics\".format(csv_path))\n",
        "  \n",
        "  df = pd.read_csv(csv_path)\n",
        "  y = df.pop('label').to_frame()\n",
        "  \n",
        "  if use_all:\n",
        "    df_X = df\n",
        "    df_y = y\n",
        "  else:\n",
        "    test_size = (100/len(df)) * num_samples\n",
        "    print(test_size)\n",
        "    _, df_X, _, df_y = sklearn.model_selection.train_test_split(df, y, stratify=y, test_size=test_size/100)\n",
        "  \n",
        "  x = np.empty(shape=(len(df_X), 1323000,1), dtype=np.float32)\n",
        "  y = []\n",
        "  \n",
        "  for i in range(len(df_X)):\n",
        "      row = df_X.iloc[i]\n",
        "\n",
        "      sound_clip,sr = librosa.load(row[0], sr=22050)\n",
        "      sound_clip = np.expand_dims(sound_clip, axis=1)\n",
        "      shape = sound_clip.shape\n",
        "      label = df_y.iloc[i]\n",
        "      label = label[0]\n",
        "      x[i] = sound_clip\n",
        "      y.append(label)\n",
        "\n",
        "  y = np.asarray(y, dtype=\"int8\")\n",
        "  print(\"Now saving to dir\")\n",
        "  np.save(tfr_dir+outname+\"_y_monitor.npy\", y)\n",
        "  np.save(tfr_dir+outname+\"_x_monitor.npy\", x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4hCoTHDXpwB"
      },
      "source": [
        "def main(args):\n",
        "  global tfr_dir\n",
        "  tfr_dir = args['output_path']\n",
        "  Path(tfr_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  gen_tfr(csv_path=args['test_list'], outname=\"test\", max_files=args['test_max'])\n",
        "  gen_tfr(csv_path=args['validation_list'], outname=\"valid\", max_files=args['valid_max'])\n",
        "  gen_tfr(csv_path=args['train_list'], outname=\"train\", max_files=args['train_max'])\n",
        "  \n",
        "  if args['use_monitoring']:\n",
        "    gen_monitoring_sample(csv_path=args['test_list'], outname=\"test\", num_samples=args['test_monitor'], use_all=args['use_all_samples'])\n",
        "    gen_monitoring_sample(csv_path=args['validation_list'], outname= \"valid\", num_samples=args['valid_monitor'], use_all=args['use_all_samples'])\n",
        "    gen_monitoring_sample(csv_path=args['train_list'], outname= \"train\", num_samples=args['train_monitor'], use_all=args['use_all_samples'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkQS3XaVXvEv"
      },
      "source": [
        "parser = argparse.ArgumentParser(description='')\n",
        "parser.add_argument('--output_path', dest='output_path', default='/content/drive/MyDrive/custaudio/tfr_dir/', help='Base path for the dataset')\n",
        "parser.add_argument('--train_list', dest='train_list', default='/content/drive/MyDrive/custaudio/custom_train.csv', help=\"CSV file that stores the training files\")\n",
        "parser.add_argument('--validation_list', dest='validation_list', default='/content/drive/MyDrive/custaudio/custom_valid.csv',help=\"CSV file that stores the validation files\")\n",
        "parser.add_argument('--test_list', dest='test_list', default='/content/drive/MyDrive/custaudio/custom_test.csv',help=\"CSV file that stores the test files\")\n",
        "parser.add_argument('--files_per_train_shard', dest='train_max', type=int, default=50, help='Number of files for the TFRecord file')\n",
        "parser.add_argument('--files_per_test_shard', dest='test_max', type=int, default=50, help='Number of files for the TFRecord file')\n",
        "parser.add_argument('--files_per_valid_shard', dest='valid_max', type=int, default=50, help='Number of files for the TFRecord file')\n",
        "\n",
        "parser.add_argument('--use_monitoring_samples', dest='use_monitoring', type=int, default=1, help='Whether to create an additional numpy array that contains samples that can be used to generate live statistics during training')\n",
        "parser.add_argument('--use_all_samples', dest='use_all_samples', type=int, default=0, help='For small datasets use all available subset samples to generate monitoring data')\n",
        "parser.add_argument('--num_train_monitor', dest='train_monitor', type=int, default=25, help='Number of train samples to store in a numpy array to observe live training statistic on')\n",
        "parser.add_argument('--num_test_monitor', dest='test_monitor', type=int, default=25, help='Number of test samples to store in a numpy array to observe live training statistics on')\n",
        "parser.add_argument('--num_valid_monitor', dest='valid_monitor', type=int, default=25, help='Number of valid samples to store in a numpy array to observe live training statistics on')\n",
        "\n",
        "args, unknown = parser.parse_known_args()\n",
        "args = args.__dict__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkADociMX1D2",
        "outputId": "f6ec4071-8add-423c-a727-daf908be24e0"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  main(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Parsing /content/drive/MyDrive/custaudio/custom_test.csv\n",
            " Using 1 shard(s) for 50 files, with up to 50 samples per shard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:14<00:00, 14.18s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Parsed 50 files for test\n",
            "Parsing /content/drive/MyDrive/custaudio/custom_valid.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Using 1 shard(s) for 50 files, with up to 50 samples per shard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:13<00:00, 13.67s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Parsed 50 files for valid\n",
            "Parsing /content/drive/MyDrive/custaudio/custom_train.csv\n",
            " Using 1 shard(s) for 50 files, with up to 50 samples per shard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:13<00:00, 13.70s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Parsed 50 files for train\n",
            "Parsing /content/drive/MyDrive/custaudio/custom_test.csv to enable logging some statistics\n",
            "50.0\n",
            "Now saving to dir\n",
            "Parsing /content/drive/MyDrive/custaudio/custom_valid.csv to enable logging some statistics\n",
            "50.0\n",
            "Now saving to dir\n",
            "Parsing /content/drive/MyDrive/custaudio/custom_train.csv to enable logging some statistics\n",
            "50.0\n",
            "Now saving to dir\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}